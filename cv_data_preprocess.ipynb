{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998abe24-16e3-451a-a2b9-849cd4df8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_path = '/home/dog4/PhD/computer_vision/VOiCES_Box_unzip/Training_Data/Automatic_Speech_Recognition/ASR_train/modified-train-clean-80'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216673af-674a-4eda-a45a-eb1e8af370fa",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071213e-9ef0-4399-8c02-6d5b7f211879",
   "metadata": {},
   "source": [
    "### Take data from my_data_path path and store it in plain directory designated by **user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ad567c-8c38-4161-9ae4-dd1065975724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a8fd0c-db2a-45d7-b672-15e1f56ca79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = '/home/dog4/PhD/computer_vision/cvproj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8cdfab-2843-444e-91b2-6985ed540f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = os.path.join(PROJECT_PATH, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdf9a56-181e-4c86-a72e-120e72765d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = 'cls0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388619b-6386-4134-809c-6b793d69bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for cls0 and cls1\n",
    "source_folder = source_folder = os.path.join(audio_path, f'{cls}/preprocessed')\n",
    "output_folder = os.path.join(audio_path, f'{cls}/spectograms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39237f24-2f68-453e-ab8c-23ef96a4b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dog4/PhD/computer_vision/cvproj/cvenv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1152\n",
      "  warnings.warn(\n",
      "/home/dog4/PhD/computer_vision/cvproj/cvenv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n",
      "/home/dog4/PhD/computer_vision/cvproj/cvenv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=640\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversion complete. 154829 spectrograms saved, 0 files skipped due to errors.\n"
     ]
    }
   ],
   "source": [
    "# === Disable plot GUI backend ===\n",
    "plt.switch_backend('Agg')\n",
    "\n",
    "# === Process each .wav file ===\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for filename in os.listdir(source_folder):\n",
    "    if not filename.lower().endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Convert to Mel Spectrogram\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # Save as monochrome image\n",
    "        fig = plt.figure(figsize=(2.24, 2.24), dpi=100)  # Gives you 224x224\n",
    "        ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "        ax.set_axis_off()\n",
    "        fig.add_axes(ax)\n",
    "        librosa.display.specshow(S_dB, sr=sr, cmap='gray')\n",
    "        out_path = os.path.join(output_folder, filename.replace(\".wav\", \".png\"))\n",
    "        plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "\n",
    "        success_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        continue\n",
    "\n",
    "print(f\"✅ Conversion complete. {success_count} spectrograms saved, {error_count} files skipped due to errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e65683-4612-48cc-94aa-a2c681cb9fa2",
   "metadata": {},
   "source": [
    "## Splitin data into folders according to their class afiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60aa469-46e7-4cdb-bb49-193808de4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit source paths for each class\n",
    "class_dirs = {\n",
    "    \"green\": \"/home/dog4/PhD/computer_vision/cvproj/data/cls1/spectograms\",\n",
    "    \"red\": \"/home/dog4/PhD/computer_vision/cvproj/data/cls0/spectograms\"\n",
    "}\n",
    "\n",
    "output_dir = \"./data/split_dataset\"\n",
    "\n",
    "# Parameters\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Create output directories\n",
    "def make_dirs():\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in class_dirs.keys():\n",
    "            os.makedirs(os.path.join(output_dir, split, cls), exist_ok=True)\n",
    "\n",
    "# Split and copy images\n",
    "def split_data():\n",
    "    make_dirs()\n",
    "\n",
    "    for cls, class_path in class_dirs.items():\n",
    "        all_files = [os.path.join(class_path, f) for f in os.listdir(class_path) if f.lower().endswith(\".png\")]\n",
    "\n",
    "        train_files, temp = train_test_split(all_files, test_size=1-train_ratio, random_state=42)\n",
    "        val_files, test_files = train_test_split(temp, test_size=test_ratio/(test_ratio + val_ratio), random_state=42)\n",
    "\n",
    "        for file in train_files:\n",
    "            shutil.copy(file, os.path.join(output_dir, \"train\", cls))\n",
    "        for file in val_files:\n",
    "            shutil.copy(file, os.path.join(output_dir, \"val\", cls))\n",
    "        for file in test_files:\n",
    "            shutil.copy(file, os.path.join(output_dir, \"test\", cls))\n",
    "\n",
    "split_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb3578-f6aa-4bdc-b73a-2b08d8ae698d",
   "metadata": {},
   "source": [
    "## Convolutional stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63c290-2232-41c4-bc7e-7e0bb51b32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Adjust for grayscale spectrograms (1 channel)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Replace the last layer with one that matches your number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d14fcd4-23ea-425f-8da4-fd7399c5a7df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cac2b-d001-4867-9277-cbaa95d6e07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
